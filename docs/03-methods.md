# Methods

## Mixed-Methods Research {-}


Formal mixed methods research designs are relatively new in social science research. According to Creswell and Plano Clark (2010), a mixed methods design collects and analyzes both quantitative and qualitative data and mixes the analyses one or more of three ways: (1) the datasets can be merged into a cohesive whole, (2) the results of one can build on the other, or (3) one dataset might be embedded in the other. Furthermore, Morse (2003) points out that mixed methods designs characteristically integrate methods that are not normally used together, such as embedding open-ended questions within Likert scale instruments. By using different types of data and analyses in a study, researchers can gain a greater depth of understanding than by using either method on its own, or, as Jick (1979) states, “Where there is convergence, confidence in the results grows considerably…However, where divergent results emerge, alternative, and likely more complex, explanations are generated” (p. 608).

Creswell and Plano Clark (2010) contend that mixed methods designs can be very effective because of the possibility of triangulating data and results. For example, if the qualitative analysis of interview transcripts can be used to corroborate the quantitative results of a survey, then the researcher has a stronger base of evidence upon which to build an argument, which can increase the validity of the mixed results. Additionally, if the qualitative and quantitative analyses yield contradictory findings, the researcher may uncover hidden complexities or be able to formulate new research hypotheses to resolve the contradiction.
Visual model of the research design.

Due to the complexity of many mixed methods designs, Creswell and Plano Clark (2010) recommend that researchers provide a visual model of their particular design. The design used in this investigation, as shown in Figure 6, was a 2-phase QUAN/QUAL concurrent triangulation model (Creswell, 2009, p. 213). The rationale for using the mixed methods approach is that the results of the two forms of data analysis could be compared and merged into an integrated analysis which would be stronger than if either a quantitative or qualitative analysis was performed in isolation. This comparison of analyses is known as triangulation or sometimes as a convergent design (Creswell & Plano Clark, 2010).

Visual Model

Figure 6. Visual Model of Research Method


## Participants {-}

Participants (n=31) in the study represented a convenience sample, as only one course that utilized this study buddy strategy was available to the researcher. A total of 101 students were invited to participate in the study; 26 in the Fall (September – December) 2012 semester, 25 students in the Winter (January – April) 2013 semester, and 50 students in two classes in the Spring (May-July) 2013 semester. All four classes had the same instructor.

## Research Design {-}

The study used a mixed methods research design involving a survey to gather quantitative and qualitative data. A quasi-experimental design was employed to compare study buddy participants’ and non-participants’ scores on the Revised Two Factor Study Process Questionnaire (R-SPQ-2F) (Biggs et al., 2001). This was followed by a basic exploratory and descriptive analysis of the remaining quantitative data, and then a phenomenological analysis of the qualitative data. An integrated analysis was then used to compare and triangulate the findings of the previous analyses. The hypothesis and null hypothesis related to the R-SPQ-2F are described below.

### Hypothesis. {-}

Participants in the study buddy activity will take a deeper approach to their learning as measured by the R-SPQ-2F when compared to non-participants.

### Null hypothesis. {-}

There will be no difference in the approaches to learning taken by study buddy participants and non-participants.

## Procedure {-}

The first round of data collection drew from the Fall 2012 and Winter 2013 cohorts and 7 responses were received, all from the Winter 2013 cohort and all had participated in the study buddy activity. The second round of data collection drew from two concurrent sections in the Spring 2013 semester. This resulted in a further 24 responses, of whom 18 participated in the study buddy activity and 6 did not. A total of 31 subjects participated in the study; 25 were participants in the study buddy activity and 6 were non-participants. The response rate was 30.6%.

## Quantitative and Qualitative Procedures {-}

### Quantitative procedure. {-}

Following data collection, responses were downloaded from LimeSurvey™ into a comma-separated file, which was opened in a spreadsheet program. Responses were divided into separate sheets according to the research questions. Because qualitative items were included among the quantitative items, a separate sheet was created for the qualitative data. Any personally identifying information was removed from the data and stored in a separate file and all study participants were assigned a code. Identifying information was only used to contact the winner of the draw.

Likert-scale items were converted from their original format to numerical responses. “Strongly Disagree” was given a score of “1” and “Strongly Agree” was given a score of “5” in accordance with the scoring scheme provided by Biggs et al. (2001). “Yes” and “No” responses were converted to “1” and “2” respectively.

The first section of the survey (the Biggs et al. R-SPQ-2F) was the only section completed by both the participants in the study buddy activity (n=25) and the non-participants (n=6). Non-participants were removed from the remaining sections of the survey data so that their blank answers would not be factored into statistical calculations.

Data were anonymized and loaded into PASW Statistics ™ (Student Version) for analysis. A limitation of the Student Version is that it is limited to 50 variables. This study contained 54 quantitative variables, so each section of the quantitative data was loaded individually. PASW Statistics ™ was used to calculate the t-test, basic descriptive statistics, and frequencies. Due to the exploratory nature of this investigation, the small sample size and the very small size of the non-participant group (n=6), further in-depth statistical analyses would have been unjustified.

### Qualitative procedure. {-}

The analysis of the qualitative data followed the hermeneutic phenomenology procedures outlined by Creswell (2007).

- Bracketing involves the researcher explaining his or her own experiences related to the phenomenon in question. This step is intended to allow the researcher to look at the phenomenon without bias or preconceived notions about the meaning of the phenomenon.
- Developing a list of significant statements through the process of horizontalization involves the researcher reading through the data several times to get a feeling for the data and then identifying statements that are particularly significant in light of the research questions. These statements are treated as having equal worth and any repeated or overlapping statements are removed from the data.
- Grouping the significant statements into themes involves the researcher identifying groups of significant statements that fall into larger categories, or themes.
- Describing what happened in the “textural description”, which outlines what happened from the perspective of the participants in the study and includes direct quotations from the participants.
- Describing how the phenomenon occurred in the “structural description”, which is a description of the context of the study.
- Combining the textural and structural description into the “composite description,” which captures the essence of the phenomenon.

Each of these steps is described more fully in Chapter V.

### Validation Procedures. {-}

Validity in qualitative research refers to the idea that the findings of a qualitative study are an accurate representation of what the participants in the study actually experienced. Creswell (2007) recommends eight strategies that can be used to ensure validity in qualitative investigations. He recommends that researchers use at least two of the eight strategies. The strategies employed in this investigation were:

- Triangulation: this investigation gathered data from multiple sources (participants, non-participants, and the instructor), gathered two types of data (quantitative and qualitative), and relied on multiple theoretical foundations (interaction, cooperative learning, and student approach to learning).
- Member checking: during the qualitative analysis, the researcher consistently checked the coding process and results against what the participants reported in the quantitative data. The results of this process are made explicit in Chapter VI.

## Instrumentation {-}

The first step of this investigation gathered both quantitative and qualitative data through a survey. Data were gathered using Biggs, Kember & Leung’s (2001) Revised Two Factor Study Process Questionnaire (R-SPQ-2F) (see Table 1), which was supplemented with additional sections designed to elicit responses related to how students perceived the effect of the study buddy activity on their approaches to learning and how they perceived the study buddy activity itself. The R-SPQ-2F is described in detail in the next section.

Open-ended questions were interspersed throughout the quantitative items on the survey. These items were designed to elicit explanations of the participants’ choices on the quantitative items in order to understand their experiences with the study buddy activity. Responses to these open-ended questions formed the qualitative data for the study. The study was proposed to include the possibility of semi-structured interviews, but it was determined after the analysis of the responses to the open-ended questions that the data obtained were sufficient to satisfy the exploratory nature of the objectives of the study.

The survey was divided into four sections corresponding to the three research questions and the fourth to gather data from subjects who did not participate in the study buddy activity. Prior to the main sections of the survey, participants were asked whether or not they participated in the study buddy activity. Those who participated were automatically directed to complete the first three sections, and those who did not participate were automatically directed to complete only the first section and the final section. The four sections are described below.

### Section 1: The Revised Two Factor Study Process Questionnaire (R-SPQ-2F). {-}

The R-SPQ-2F is predicated on the idea that students may take either a deep or a surface approach to different learning tasks depending on several factors as outlined in the 3P model of teaching and learning (Biggs et al., 2001). The R-SPQ-2F consists of 20 5-point Likert scale items, which are designed to gauge how an individual student approaches a particular learning task, with the goal of identifying whether the student takes a deep or a surface approach to the learning task. There are 10 items related to each approach. In addition to the two main scales, there are four subscales measured by the R-SPQ-2F. Within each scale are the two subscales related to the strategies students use and to their motives for using the particular approach. The R-SPQ-2F can be scored to reflect either the two main scales of a deep approach (DA) or a surface approach (SA) or to reflect the subscales, which are deep motive (dm), deep strategy (ds), surface motive (sm) or surface strategy (ss). Table 1 shows how the survey items align with each of the scales and subscales.

Table 1

Biggs et al. (2001) calculated Cronbach’s alpha (), which provides a measure for how reliably an instrument measures a particular phenomenon. Values for  can range from 0 to 1, with higher scores indicating higher reliability. Biggs et al. calculated values for the R-SPQ-2F scales at 0.73 for the deep approach items and 0.64 for the surface approach items, values which are considered acceptable.

In response to the suggestion from Biggs et al. that the instrument may be more sensitive if some items are revised according to different learning contexts, items 17 through 20 were revised to remove references to face-to-face classrooms and examinations as neither of those elements were features of the course used in the study.

In addition to the R-SPQ-2F questionnaire, participants were asked to rate their responses on two additional categories of questions. The first category of questions was aimed at determining how study buddy participants think that the study buddy activity affected their learning based on Slavin’s (2011) integrated theoretical model of cooperative learning processes (Figure 4). The final category of questions was related to participants’ perceptions of the logistics of the study buddy activity and their evaluation of the structure of the activity.

Students who chose not to participate in the study buddy activity completed section 1 of the survey related to their approach to learning, and then were directed to the final section, a series of questions to gauge their views on why they didn’t participate and under what conditions they might choose to participate in the future.

One option on one item was added after the survey was administered to the Fall 2012 and Winter 2013 classes. The original survey asked participants if they would recommend the study buddy activity with the following options for responses: (1) to other learners in MDDE 604, (2) for use in other MDDE courses, or (3) for use as a general distance education strategy. After the Winter 2013 round of data collection, a fourth option was added to the question, (4) I would not recommend this activity for other learners or courses.

### Section 2: Exploring Slavin’s integrated model. {-}

Table 2 shows the second category of questions and how they are aligned with the second research question: As a cooperative learning activity, does the study buddy activity provide sufficient scaffolding to promote deep approaches to learning?

Categories of questions were derived from Slavin’s (2011) integrated model of cooperative learning. According to Slavin, there are four theoretical perspectives that interdependently explain how cooperative learning activities enhance learning. Theorists from the motivational perspective suggest that cooperative learning activities provide high levels of task motivation for participants to complete the required work. From the social cohesion perspective, students are motivated by their affinity for their group mates. The motivationalist and social cohesion perspectives work together in a mutually reinforcing feedback loop to enhance the effect of the activity. There are two perspectives that are considered cognitive perspectives. The cognitive development perspective suggests that students in cooperative learning environments are provided many opportunities to be challenged in what Vygotsky (1978) calls the zone of proximal development, where students are exposed to developmentally appropriate challenges. The cognitive elaboration, or cognitive restructuring, perspective posits that learning is enhanced when participants in cooperative learning activities are exposed to opportunities to consider their preconceptions and misconceptions of ideas in light of new information and to form more accurate models of the world.

In this investigation, participants provided self-reports on the four categories of learning effects. There were two or three items in this section of the survey for each theoretical perspective on cooperative learning activities. Each item was answered on a 5-point Likert scale, ranging from (1) “Strongly disagree” to (5) “Strongly agree.”

Table 2

Participants were also asked whether the study buddy activity helped them to improve in various areas and if they would recommend the activity to others. These items were answered with either “Yes” or “No” (Table 3).

Table 3

### Section 3: Exploring student perceptions of the structure of the study buddy. {-}

The third section of the survey (Table 4) was used to determine how participants perceived the logistical structure and requirements of the study buddy
activity. The questions in this section were answered on a 5-point Likert scale, ranging from (1) “Strongly disagree” to (5) “Strongly agree”.

Table 4

Participants were asked about the quantity and quality of their interaction with their study buddy partner, as well as their views on how the activity was structured in the course (Table 5).

Table 5

### Section 4: Exploring the views of non-participants. {-}

Participants who reported that they did not participate in the study buddy activity were directed to a brief section of questions asking them for explanations of why they chose to not participate and what it might take for them to participate in a similar activity in the future (Table 6).

Table 6

## Data Collection {-}

The survey was administered and responses collected through LimeSurvey™, an open source online survey tool hosted on a server at Athabasca University. Students in the course MDDE 604 were sent an email (Appendix 2) inviting them to participate in the survey. The email contained information about the purpose of the survey, how long the survey would take, that participants would be eligible for a draw for a $100 gift card, and that participation was entirely voluntary. The email explained that their instructor was one of the supervisors of the thesis investigation but that he would not know whether or not students had participated in the study, nor would he have access to the raw data and only to the aggregated data after the course had ended and the grades had been submitted. Students were instructed that the thesis investigation had been reviewed and approved by the Athabasca University Research Ethics Board and that they would be indicating their informed consent by clicking the link to the survey.

With the exception of the Fall 2012 class, who were invited after the course had ended, students were sent the invitation approximately half-way through the course and were sent reminders after they submitted the last assignment and immediately prior to the end of the course.

## Data Analysis {-}

### Quantitative analysis. {-}

Quantitative data were analyzed using an independent samples t-test for the R-SPQ-2F section of the survey and basic descriptive statistics for the remainder of the quantitative data.

An independent samples t-test was performed to determine whether or not there were any statistically significant differences in student approaches to learning between those subjects who participated in the study buddy activity and those who did not participate in the study buddy activity.

It was possible to score between 10 and 50 points on each of the two scales measured by the R-SPQ-2F. For example, students who took a particular approach (deep or surface) about half the time would score 30 points on the corresponding scale and those who frequently took a particular approach would score 40 points on the corresponding scale.

Biggs et al. (2001) do not provide or recommend norms or standards for their instrument because of the high degree of variability of institutional and teaching contexts (presage factors). Instead, they recommend the development of norms within institutions or even individual courses. As such, this study, being the first to examine this activity with the R-SPQ-2F, could not compare students’ scores with any previously published norms. Therefore, for this investigation, those participants who scored more than 40 points on the deep scale and less than 20 points on the surface scale were considered to have taken a predominantly deep approach in their learning. Those who scored 40 or fewer points on the deep scale and 20 or more points on the surface scale were considered to have taken a predominantly surface approach.

### Qualitative analysis. {-}

Qualitative data were analyzed according to the phenomenological protocols for analyzing qualitative data as outlined in Creswell (2007). Phenomenology is the study of the lived experiences of humans and is based largely on the ideas of Edmund Husserl, a German mathematician (Moustakas, 1994; van Manen, 1990). Contrary to quantitative methods, which seek to dichotomize, explain, and predict, phenomenology seeks to understand human experience (van Manen, 1990).

According to Creswell’s (2007) protocol, the first task of phenomenologists is to describe their experience with the phenomenon in a process called bracketing. This process helps the researcher set aside his or her own experience and analyze the phenomenon from a new perspective. The researcher then reads through the data to develop a list of significant statements, which are then reduced to a list of non-overlapping statements through the process of horizontalization. These statements are then grouped into themes or meaning units. The next step is to write a textural description of the phenomenon, which essentially answers the question “What happened?” This is followed by the structural description, which describes how the phenomenon occurred and includes a description of the larger context or setting of the phenomenon. The final step is to write a composite description, which is usually a long paragraph integrating the textural and structural descriptions into a description of the essence of the phenomenon.
Merging the findings.

The final step of the analysis was to compare the results of the quantitative and qualitative analyses into a single, unified statement with respect to what the findings revealed in light of the research questions and the recommendations regarding incorporating the study buddy activity into online distance learning course design. The two analyses were integrated to show areas of convergence and divergence in a process known as triangulation (Creswell & Plano Clark, 2010; Jick, 1979).

## Ethical Considerations {-}


This thesis investigation was reviewed and approved by the Athabasca University Research Ethics Board. There was a possibility that participants in the study could have been inappropriately compelled to participate in the study or to provide answers to the survey questions that did not accurately reflect their views because one of the supervisors of the research was also the instructor of the course being investigated. It was necessary for the researcher to investigate this particular course because it was the only known course that utilized the study buddy activity.

The following steps were taken to ensure that the participants’ decision whether or not to participate in the study and their answers to the survey questions were not influenced by their relationship to the co-supervisor:

- participant recruitment was initiated by Athabasca University support staff,
- the co-supervisor never knew which students chose to participate or not, or if any students withdrew from the activity,
- all identifying information was redacted from the quantitative and qualitative data prior to the co-supervisor having access,
- the co-supervisor did not have access to the redacted data until after the course was completed and all grades were submitted to the university.

## Chapter Summary {-}

This chapter outlined the research methodology for this investigation, including a description of the general characteristics of mixed methods research. The design of this thesis investigation was described as a 2-phase QUAN/QUAL concurrent triangulation model, with the rationale that the 2 phases of data analysis would be compared and merged into a coherent whole that was stronger than if either method had been used in isolation. The next sections provided descriptions of the participants in the study, the data collection procedures, and the instrument used to collect the data. The data collection survey was divided into four sections, one for each of the research questions and one final section for those students who did not participate in the study buddy activity. The next section of chapter three described how the quantitative and qualitative data were analyzed and merged. The chapter ended with a description of the ethical considerations and the review and approval by the Athabasca University Research Ethics Board.

